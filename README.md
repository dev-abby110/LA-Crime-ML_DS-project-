# Los Angeles Crime Analysis (2020–2024)

Comprehensive exploratory data analysis (EDA), clustering, and machine learning experiments on the Los Angeles crime dataset (2020–present). This repository contains data-cleaning notebooks, visual EDA, clustering experiments (K-Means vs DBSCAN), deep learning (ANN & LSTM) experiments, and several classical ML model comparisons.

## Table of contents

- Project overview
- Repository structure
- Data (source and notes)
- Notebooks — detailed summaries
- Artifacts and outputs produced by the notebooks
- How to reproduce / run locally (Windows PowerShell examples)
- Recommended environment / requirements
- Contract (inputs / outputs / success criteria)
- Edge cases and limitations
- Quick quality gates summary
- Next steps and ideas
- License & contact

## Project overview

The notebooks in this repo focus on cleaning and exploring Los Angeles crime records, extracting temporal and spatial features, clustering incidents to identify patterns/hotspots, and building predictive models to classify crime types (e.g., violent vs non-violent). The work includes:

- Data cleaning and feature engineering
- Exploratory visualizations (temporal, spatial, demographics)
- Clustering experiments (K-Means and DBSCAN) with metrics and visualizations
- Supervised learning experiments: ANN (dense model), LSTM (sequence model), and classical ML models (Decision Tree, Random Forest, KNN, Naive Bayes)
- Geospatial visualizations using Folium (heatmaps and area bubble maps)

## Repository structure

Files in the repository root (what I found):

- `cleaned_crime_data.csv` — cleaned dataset output produced by notebooks (large CSV generated by the ANN notebook).
- `clustering.ipynb` — clustering workflow: preprocessing, feature selection, PCA, K-Means, DBSCAN, metrics and visualizations.
- `data-cleaning-eda.ipynb` — thorough data cleaning and EDA (missing values, temporal features, statistics, plots) and writes `cleaned_crime_data.csv`.
- `DL.ipynb` — deep learning experiments (ANN for classification, LSTM for sequence modeling). Trains ANN, saves `crime_prediction_ann_model.h5`, `model_results_summary.txt`, and visualizations.
- `LA_CRIME_EDA.ipynb` — additional exploratory analysis, geospatial visualizations (folium heatmaps, choropleth-like bubble maps), time-series decomposition and summary report.
- `models-la-crimedata (1).ipynb` — classical ML models and comparison (Decision Tree, Random Forest, KNN, Naive Bayes) plus visual comparisons and ROC/PR analyses.

Note: Filenames may contain spaces. For reproducibility, reference them using quotes or escaped paths in scripts.

## Data — source & notes

Source / Dataset used in notebooks:
- Kaggle dataset (referenced in notebooks): `shubhamgupta012/crime-data-from-2020-to-present` (notebooks use a `kagglehub` helper to download; you can instead use the official Kaggle CLI/API).

Key columns (examples used across notebooks):
- `DR_NO` (record id)
- `Date Rptd`, `DATE OCC`, `TIME OCC` (report/occurred timestamps)
- `AREA`, `AREA NAME`, `Rpt Dist No` (location / area)
- `Crm Cd`, `Crm Cd Desc` (crime codes and descriptions)
- `Vict Age`, `Vict Sex`, `Vict Descent` (victim demographics)
- `LAT`, `LON` (geolocation)

Important notes about the dataset:
- The original dataset is large (500k–1M rows depending on source). Notebooks apply sampling or filtering when needed (e.g., clustering samples to 50k rows for performance).
- Several notebooks drop columns with high missingness (>30% or >50% depending on notebook).
- Notebooks generate `cleaned_crime_data.csv` for downstream modeling and mapping.
- Some notebooks rely on `kagglehub` in the code to fetch datasets — if you prefer, use the Kaggle CLI (`kaggle datasets download`) or manually download the CSV and place it under `data/`.

## Notebooks — detailed summaries

Below are concise, descriptive summaries of each notebook and the main steps they perform.

### `data-cleaning-eda.ipynb`
Purpose: Clean the raw CSV and perform exploratory data analysis.
Key steps:
- Load raw CSV (path from Kaggle input in the original environment).
- Drop duplicate rows, remove columns with many missing values, and impute remaining missing values (mode for categorical, median for numeric).
- Fix `TIME OCC` formatting to `HH:MM`, convert date columns to datetime.
- Remove rows with invalid coordinates or extreme missingness, filter invalid victim values.
- Extract temporal features: year, month, day, hour, day_of_week, is_weekend.
- Visual EDA: histograms, boxplots, top crime types, area distributions, geospatial scatter plots, and a Folium bubble map/heatmap.
- Save cleaned dataset: `cleaned_crime_data.csv`.

Why read it: It's the canonical cleaning script used by the other notebooks and produces the cleaned CSV used by models.

### `LA_CRIME_EDA.ipynb`
Purpose: A rich EDA & geospatial analysis with clear reporting and many plots.
Key steps:
- Data cleaning similar to the previous notebook: standardize column names, drop columns with >30% missingness, impute with modes.
- Extract features (year/month/day/hour) and produce many plots: crime trends per year, day-of-week distribution, top crimes, area distributions, monthly trends of top crimes.
- Geospatial: Folium heatmaps and area bubble maps; separate maps for male vs female victims using MarkerCluster.
- Time series decomposition (daily crime counts) and a long markdown report summarizing findings and next steps.

Why read it: Great narrative and many reproducible visualizations; useful if you want public-facing maps and charts.

### `clustering.ipynb`
Purpose: Compare K-Means vs DBSCAN for crime clustering; produce cluster-level interpretations.
Key steps:
- Downloads dataset, cleans, samples (50k records if dataset large).
- Label-encodes categorical columns with limited cardinality, drops high-cardinality text.
- Standardizes numeric features, applies PCA (2 components) for visualization.
- K-Means: elbow method / silhouette scores loop to choose K; compute silhouette, Davies–Bouldin, Calinski–Harabasz; visualize clusters in PCA space.
- DBSCAN: choose eps using k-distance graph, train, count noise points; compute metrics where valid.
- Cluster interpretation: show top crime types, average hour, weekend% and area name for clusters.
- Visuals: metric comparison, cluster size bars, temporal patterns per cluster, crime-type heatmap per cluster.

Why read it: If your goal is to identify natural groupings (hotspots or behavior clusters) this notebook is the primary resource.

### `DL.ipynb`
Purpose: Deep learning experiments — ANN for multi-class classification and LSTM for sequence modeling.
Key steps (ANN):
- Clean data, choose a target variable (looks for `Crm Cd Desc` or similar), limit to top N classes (commonly top 10) for tractability.
- Label-encodes categorical features, scales numeric features, splits into train/test.
- Builds an ANN with multiple dense layers, dropout, compiles with sparse categorical crossentropy.
- Trains with early stopping; saves `crime_prediction_ann_model.h5`, `cleaned_crime_data.csv`, `crime_ann_analysis_results.png`, and `model_results_summary.txt`.

Key steps (LSTM):
- Prepares temporal sequences (sequence length e.g., 10), encodes labels, scales features with MinMaxScaler.
- Builds stacked LSTM model for sequence classification and trains with early stopping and LR reduction callbacks.

Why read it: If you want the trained ANN model or baseline deep-learning approach, this is the place.

### `models-la-crimedata (1).ipynb`
Purpose: Classical ML model comparisons for predicting crime type (violent vs non-violent) and related labels.
Key steps:
- Feature engineering (hour/month/area/victim age/premise/weapon features), label creation for binary `crime_type` (violent/nonviolent).
- Data cleaning and outlier removal for Victim Age.
- Train & evaluate Decision Tree, Naive Bayes (hybrid approach), Random Forest, KNN, and compute metrics (accuracy, confusion matrices, ROC curves).
- Visual comparisons of accuracy, confusion matrices, feature importance for RF, decision boundary visualization using PCA for 2D.

Why read it: Good comparison of interpretable models; useful for quick baselines and feature importance exploration.

## Artifacts / outputs produced by the notebooks

- `cleaned_crime_data.csv` — produced by notebooks (cleaned dataset used across experiments). This file can be large (hundreds of MB). If the repo includes it, be mindful of GitHub file-size limits.
- `crime_prediction_ann_model.h5` — ANN model saved from `DL.ipynb`.
- `crime_ann_analysis_results.png` — training history, confusion matrices and other visualizations.
- `model_results_summary.txt` — textual summary of model evaluation metrics (ANN notebook).
- Notebook-generated maps (Folium HTML files) and saved PNGs for figures.

If you plan to include these artifacts in the repository, prefer to store large models/datasets in releases or external storage (e.g., S3, Google Drive, or Git LFS) and provide download instructions in this README.

## How to reproduce / run locally (Windows PowerShell examples)

1) Clone the repository and open your shell in the repo root.

2) (Recommended) Create a Python virtual environment and install dependencies.

PowerShell commands:

```powershell
# create venv (Windows PowerShell)
python -m venv .venv
# activate it
.\.venv\Scripts\Activate.ps1
# upgrade pip
python -m pip install --upgrade pip
# install requirements (see section below)
python -m pip install -r requirements.txt
```

If you don't have `requirements.txt` provided, install the main packages used in notebooks manually:

```powershell
python -m pip install pandas numpy matplotlib seaborn scikit-learn jupyterlab jupyter folium tensorflow statsmodels
# optionally: geopandas contextily if you want advanced maps
python -m pip install geopandas contextily
```

3) Obtain the dataset (either download from Kaggle or place your `Crime_Data_from_2020_to_Present.csv` under `data/`):

- Option A: Use Kaggle CLI (recommended)
  - Install Kaggle CLI and configure `kaggle.json` API token.
  - Download dataset referenced in notebooks (example used by notebooks: `shubhamgupta012/crime-data-from-2020-to-present`):

```powershell
# Example using kaggle (notebook used 'kagglehub' but kaggle CLI is standard)
kaggle datasets download -d shubhamgupta012/crime-data-from-2020-to-present -p data/ --unzip
```

- Option B: Manually download CSV from Kaggle UI and put the CSV in `data/` or repo root.

4) Launch Jupyter / VS Code and run the notebooks in this order for a clean pipeline:

- `data-cleaning-eda.ipynb` (run to generate cleaned data)
- `LA_CRIME_EDA.ipynb` (explore & produce maps/plots)
- `clustering.ipynb` (cluster analysis)
- `models-la-crimedata (1).ipynb` (classical models)
- `DL.ipynb` (ANN & LSTM — training can be slow; adjust epochs or sample data)

Notes:
- For training large models (ANN/LSTM), use a machine with sufficient RAM and GPU (optional) if available.
- If your machine is limited, edit cells to sample subset of the data (e.g., df.sample(n=50000)).

## Recommended environment / requirements

Create a `requirements.txt` with the following (example minimal):

```
pandas
numpy
matplotlib
seaborn
scikit-learn
jupyterlab
jupyter
folium
statsmodels
tensorflow
geopandas
contextily
```

Notes:
- `tensorflow` can be heavy. If you only want to run EDA and classical ML, omit TensorFlow.
- If you cannot install `geopandas` easily on Windows, you can skip geospatial mapping or follow geopandas installation docs (conda is often simpler for the geospatial stack).
- Some notebooks import `kagglehub` — this is not a standard package in PyPI. Replace those calls with the Kaggle CLI or manual file placement.

## Contract (short)

- Inputs: raw CSV (Crime_Data_from_2020_to_Present.csv) or `cleaned_crime_data.csv` produced by notebooks.
- Outputs: cleaned CSV, visualizations (PNG/interactive HTML), trained models (HDF5), model metrics files.
- Success criteria: Notebooks run without error on the provided data; cleaned CSV generated; key visualizations are reproducible.

## Edge cases & limitations

- Large CSV: Notebooks assume ability to load large CSVs into memory. If you have low memory, sample or use chunked processing.
- Missing `kagglehub` or Kaggle credentials: notebooks that download data will fail; use manual download as fallback.
- Non-deterministic outputs: clustering/ML results can vary; notebooks often set random seeds (e.g., random_state=42) but consider reproducibility for experiments involving GPU nondeterminism.
- Column name differences: notebooks reference column variants (`Crm Cd Desc`, `crime_code_description`, etc.). If you change column names, update the notebook cells accordingly.

## Quick quality gates summary

- Build: PASS — Notebooks don't require a compiled build step. Reproducibility depends on Python environment and dataset availability.
- Lint/Typecheck: NOT RUN — Notebooks are exploratory; consider extracting reusable code into .py modules and running flake8/mypy for stricter checks.
- Tests: NONE — There are no automated tests. Recommended: add a tiny unit test for the data cleaning pipeline (validate column names and row counts on a small sample).

## Next steps / suggestions

- Extract core cleaning and feature engineering into a single Python module (e.g., `src/data/cleaning.py`) and write unit tests.
- Add a small sample CSV (`data/sample_crime_data.csv`) for CI-friendly tests and example runs.
- Move large artifacts (models, large CSV) to Git LFS or external storage and provide download links in the README.
- Add argument-driven scripts for reproducible runs (e.g., `scripts/run_clustering.py --sample 50000`).
- If deploying models, produce a minimal Flask/FastAPI wrapper and provide prediction examples.

## License & contact

- This repository does not include a license file. Add an appropriate license (MIT, Apache-2.0, etc.) depending on how you want the code to be used.

---

If you'd like, I can:
- Create a `requirements.txt` filled from notebooks' imports.
- Produce a small `sample_crime_data.csv` (1000 rows) so the notebooks can run in CI/tests.
- Extract the cleaning pipeline into a reusable script and add basic unit tests.


Tell me which of those you'd like me to do next and I'll implement it.
